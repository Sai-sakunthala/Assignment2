{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnqWJv7OAOvIp4VNsv/2Mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sai-sakunthala/Assignment2/blob/main/Assignment_2_partA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6uHpe1KCp5n"
      },
      "outputs": [],
      "source": [
        "#install pytorch\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as functional\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import pytorch_lightning as pl\n",
        "from torchvision import transforms, datasets\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import wandb"
      ],
      "metadata": {
        "id": "sRqKFdJFCvhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(pl.LightningModule):\n",
        "    def __init__(self, initial_in_channels=3, num_classes=10, num_conv_layers=5, num_filters=64, kernel_size=3, activation_fn=nn.SiLU,\n",
        "                 dense_neurons=256, learning_rate=1e-3, use_batchnorm=True, dropout_rate=0.3, filter_organization='same', data_augmentation = True):\n",
        "\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # initialize a list to save all convolution layers\n",
        "        layers_conv = []\n",
        "\n",
        "        #number of imput images channels which is 3 in our case\n",
        "        input_channels = initial_in_channels\n",
        "\n",
        "        #variable to track filters in current layer\n",
        "        current_filters = num_filters\n",
        "\n",
        "        #loop over number of convolution layers\n",
        "        for i in range(num_conv_layers):\n",
        "            #number of output channels needed\n",
        "            out_channels = current_filters\n",
        "\n",
        "            #convolution layer with padding\n",
        "            layers_conv.append(nn.Conv2d(input_channels, out_channels, kernel_size = kernel_size, padding = kernel_size//2))\n",
        "\n",
        "            #if batch normalization is specified use it\n",
        "            if use_batchnorm:\n",
        "                layers_conv.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "            #activation layer\n",
        "            layers_conv.append(activation_fn())\n",
        "\n",
        "            #dropout is added after activation layer\n",
        "            if dropout_rate == 0:\n",
        "                layers_conv.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "            #maxpool layer\n",
        "            layers_conv.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "            #update input channels\n",
        "            input_channels = out_channels\n",
        "\n",
        "            #update number of filters for following layers based on configuration\n",
        "            if filter_organization == 'double':\n",
        "                current_filters *= 2\n",
        "            elif filter_organization == 'half':\n",
        "                current_filters = max(4, current_filters // 2)\n",
        "\n",
        "        #add all layers as convolution block\n",
        "        self.conv_block = nn.Sequential(*layers_conv)\n",
        "\n",
        "        #dense layer\n",
        "        self.fc1 = nn.LazyLinear(dense_neurons)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(dense_neurons) if use_batchnorm else None\n",
        "        self.activation_dense = activation_fn()\n",
        "        self.dropout_fc1 = nn.Dropout(dropout_rate) if dropout_rate == 0 else None\n",
        "\n",
        "        #final classification layer\n",
        "        self.fc2 = nn.Linear(dense_neurons, num_classes)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        #forward propagation of network\n",
        "        x = self.conv_block(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        if self.hparams.use_batchnorm:\n",
        "            x = self.bn_fc1(x)\n",
        "        x = self.activation_dense(x)\n",
        "        if self.hparams.dropout_rate == 0:\n",
        "            x = self.dropout_fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        #training in batches\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = functional.cross_entropy(y_hat, y)\n",
        "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
        "\n",
        "        #log metrics\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        #validation in batches\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = functional.cross_entropy(y_hat, y)\n",
        "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
        "\n",
        "        #log metrics\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        #useful for testing the model later\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = functional.cross_entropy(y_hat, y)\n",
        "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
        "\n",
        "        #log metrics\n",
        "        self.log(\"test_loss\", loss, prog_bar=True)\n",
        "        self.log(\"test_acc\", acc, prog_bar=True)\n",
        "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #adam optimizer with weightdecay\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay = 5e-5)\n",
        "\n",
        "        #learning rate scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "        return [optimizer], [scheduler]"
      ],
      "metadata": {
        "id": "UxAKVB5GC7nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sweep configuration with wandb\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'val_acc',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'num_filters': {'values': [64]},\n",
        "        'activation_fn': {'values': ['GELU', 'SiLU', \"Mish\"]},\n",
        "        'filter_organization': {'values': ['same', 'double']},\n",
        "        'use_batchnorm': {'values': [True]},\n",
        "        'dropout_rate': {'values': [0.3]},\n",
        "        'dense_neurons': {'values': [256, 512]},\n",
        "        'learning_rate': {'values': [1e-3]},\n",
        "        'batch_size': {'values': [64]},\n",
        "        'data_augmentation': {'values': [True]},\n",
        "        'kernel_size': {'values': [3]},\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "zwxJjyiQC8rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define activation functions\n",
        "def get_activation(name):\n",
        "    return {\n",
        "        \"ReLU\": nn.ReLU,\n",
        "        \"GELU\": nn.GELU,\n",
        "        \"SiLU\": nn.SiLU,\n",
        "        \"Mish\": nn.Mish\n",
        "    }[name]\n",
        "\n",
        "#train function\n",
        "def train(config=None):\n",
        "    with wandb.init(config=config) as run:\n",
        "        #for reproducibility of results\n",
        "        random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        #load configuration from sweep\n",
        "        config = wandb.config\n",
        "\n",
        "        #rename runs\n",
        "        run.name = f\"{config.activation_fn}_{config.filter_organization}_r{config.dropout_rate}_fc{config.dense_neurons}\"\n",
        "        run.save()\n",
        "\n",
        "        #wandb logger\n",
        "        wandb_logger = WandbLogger(project=\"cnn-sweep\", log_model='all')\n",
        "\n",
        "        #augmentation of data if required\n",
        "        if config.get(\"data_augmentation\", False):\n",
        "            transform_list = [\n",
        "                transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.Resize((128, 128)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ]\n",
        "        else:\n",
        "            transform_list = [\n",
        "                transforms.Resize((128, 128)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ]\n",
        "\n",
        "        #non augmented data for validation\n",
        "        val_transform = val_transform = transforms.Compose([\n",
        "    \t\t\ttransforms.Resize((128, 128)),\n",
        "    \t\t\ttransforms.ToTensor(),\n",
        "        \t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "        \t\t])\n",
        "\n",
        "        transform = transforms.Compose(transform_list)\n",
        "\n",
        "        #load training data\n",
        "        data_dir = \"/root/inaturalist_12K/train\"\n",
        "        full_dataset = datasets.ImageFolder(root=data_dir)\n",
        "        num_classes = len(full_dataset.classes)\n",
        "\n",
        "        #convert each class to index\n",
        "        class_to_indices = defaultdict(list)\n",
        "        for idx, (_, label) in enumerate(full_dataset.samples):\n",
        "            class_to_indices[label].append(idx)\n",
        "\n",
        "        #list for splitting to train and val indices\n",
        "        train_indices = []\n",
        "        val_indices = []\n",
        "\n",
        "        #get indices\n",
        "        for label, indices in class_to_indices.items():\n",
        "            random.shuffle(indices)\n",
        "            split = int(0.8 * len(indices))\n",
        "            train_indices.extend(indices[:split])\n",
        "            val_indices.extend(indices[split:])\n",
        "\n",
        "        random.shuffle(train_indices)\n",
        "\n",
        "        #load train and val datasets\n",
        "        train_dataset = Subset(datasets.ImageFolder(root = data_dir, transform = transform), train_indices)\n",
        "        val_dataset = Subset(datasets.ImageFolder(root = data_dir, transform = val_transform), val_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(val_dataset, config.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        class_names = full_dataset.classes\n",
        "\n",
        "        #initialize model with wandb configurations\n",
        "        model = CNN(\n",
        "            initial_in_channels=3,\n",
        "            num_classes=num_classes,\n",
        "            num_conv_layers=5,\n",
        "            num_filters=config.num_filters,\n",
        "            kernel_size=config.kernel_size,\n",
        "            activation_fn=get_activation(config.activation_fn),\n",
        "            dense_neurons=config.dense_neurons,\n",
        "            learning_rate=config.learning_rate,\n",
        "            use_batchnorm=config.use_batchnorm,\n",
        "            dropout_rate=config.dropout_rate,\n",
        "            filter_organization=config.filter_organization,\n",
        "            data_augmentation=config.data_augmentation\n",
        "        )\n",
        "\n",
        "        # Add callbacks\n",
        "        callbacks = [\n",
        "            #pl.callbacks.EarlyStopping(monitor=\"val_acc\", patience=5),\n",
        "            pl.callbacks.ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1)\n",
        "        ]\n",
        "\n",
        "        #train model\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=15,\n",
        "            precision=16,\n",
        "            logger=wandb_logger,\n",
        "            accelerator=\"gpu\",\n",
        "            devices=1,\n",
        "            callbacks=callbacks,\n",
        "            gradient_clip_val=0.5\n",
        "        )\n",
        "        try:\n",
        "            trainer.fit(model, train_loader, val_loader)\n",
        "        finally:\n",
        "            wandb.finish()\n",
        "\n",
        "#sweep over all configurations\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"cnn-sweep\")\n",
        "wandb.agent(sweep_id, function=train, count=1)"
      ],
      "metadata": {
        "id": "9Ry9fsQrDBWJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}