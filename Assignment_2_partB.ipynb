{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sai-sakunthala/Assignment2/blob/main/Assignment_2_partB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "ED_DxE_yKFut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anv2qAdBcOCG"
      },
      "outputs": [],
      "source": [
        "#import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import wandb\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTunedModel(pl.LightningModule):\n",
        "    def __init__(self, num_classes=10, freeze_k = 2, unfreeze_every=2, dropout_prob = 0.4):\n",
        "        super(FineTunedModel, self).__init__()\n",
        "\n",
        "        #loading EfficientNet_V2_M model\n",
        "        self.model = efficientnet_v2_m(weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        #initial freeze blocks\n",
        "        self.freeze_k = freeze_k\n",
        "\n",
        "        #after how many epochs the unfreezing happens\n",
        "        self.unfreeze_every = unfreeze_every\n",
        "\n",
        "        #total number of blocks\n",
        "        self.total_blocks = len(self.model.features)\n",
        "\n",
        "        #freeze k blocks\n",
        "        for i, block in enumerate(self.model.features):\n",
        "            if i < freeze_k:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        #add dropout and change the final classification layer\n",
        "        self.model.classifier[1] = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_prob),\n",
        "            nn.Linear(self.model.classifier[1].in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #adam with weight decay and lr scheduler\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=5e-5, weight_decay=5e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        #train in batches\n",
        "        images, labels = batch\n",
        "        outputs = self.model(images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "        #log metrics\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_accuracy', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        #validation in batches\n",
        "        images, labels = batch\n",
        "        outputs = self.model(images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "        #log metrics\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_accuracy', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        #unfreezing after unfreeze_every epochs\n",
        "        if self.current_epoch % self.unfreeze_every == 0:\n",
        "\n",
        "            #new k after unfreezing\n",
        "            new_k = self.freeze_k + self.current_epoch // self.unfreeze_every\n",
        "\n",
        "            #make required gradient as true for newly unfrozen layers\n",
        "            if new_k > self.freeze_k and new_k < self.total_blocks:\n",
        "                for i in range(self.freeze_k, new_k + 1):\n",
        "                    for param in self.model.features[i].parameters():\n",
        "                        param.requires_grad = True\n",
        "\n",
        "                #update freeze_k\n",
        "                self.freeze_k = new_k + 1"
      ],
      "metadata": {
        "id": "iZwp1ckc4pr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZCU3yJQFEbR"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "        #for reproducibility\n",
        "        random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        #initialize wandb project\n",
        "        wandb.init(project=\"inaturalist_finetune\", name=\"efficient_net_4\")\n",
        "        wandb_logger = WandbLogger(project=\"inaturalist_finetune\", name=\"efficient_net_4\")\n",
        "\n",
        "        #augment data and resize to fit the efficientnet image dimentions\n",
        "        transform_list = [\n",
        "                          transforms.Resize((224, 224)),\n",
        "                          transforms.RandomHorizontalFlip(),\n",
        "                          transforms.RandomResizedCrop(224),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                          ]\n",
        "\n",
        "        transform = transforms.Compose(transform_list)\n",
        "\n",
        "        #non augmented data for validation\n",
        "        val_transform = val_transform = transforms.Compose([\n",
        "    \t\t\ttransforms.Resize((128, 128)),\n",
        "    \t\t\ttransforms.ToTensor(),\n",
        "        \t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "        \t\t])\n",
        "\n",
        "        #load training data\n",
        "        data_dir = \"/root/inaturalist_12K/train\"\n",
        "        full_dataset = datasets.ImageFolder(root=data_dir)\n",
        "        num_classes = len(full_dataset.classes)\n",
        "\n",
        "        #convert each class to index\n",
        "        class_to_indices = defaultdict(list)\n",
        "        for idx, (_, label) in enumerate(full_dataset.samples):\n",
        "            class_to_indices[label].append(idx)\n",
        "\n",
        "        #list for splitting to train and val indices\n",
        "        train_indices = []\n",
        "        val_indices = []\n",
        "\n",
        "        #get indices\n",
        "        for label, indices in class_to_indices.items():\n",
        "            random.shuffle(indices)\n",
        "            split = int(0.8 * len(indices))\n",
        "            train_indices.extend(indices[:split])\n",
        "            val_indices.extend(indices[split:])\n",
        "\n",
        "        random.shuffle(train_indices)\n",
        "\n",
        "        #load train and val datasets\n",
        "        train_dataset = Subset(datasets.ImageFolder(root = data_dir, transform = transform), train_indices)\n",
        "        val_dataset = Subset(datasets.ImageFolder(root = data_dir, transform = val_transform), val_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, 64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(val_dataset, 64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        class_names = full_dataset.classes\n",
        "\n",
        "        #initialize model with our required configurations\n",
        "        model = FineTunedModel(num_classes, 2, 2, 0.4)\n",
        "\n",
        "        #add callbacks\n",
        "        callbacks = [\n",
        "            #pl.callbacks.EarlyStopping(monitor=\"val_acc\", patience = 5),\n",
        "            pl.callbacks.ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1)\n",
        "        ]\n",
        "\n",
        "        #train model\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=25,\n",
        "            precision=16,\n",
        "            logger=wandb_logger,\n",
        "            accelerator=\"gpu\",\n",
        "            devices=1,\n",
        "            callbacks=callbacks,\n",
        "            gradient_clip_val=0.5\n",
        "        )\n",
        "        try:\n",
        "            trainer.fit(model, train_loader, val_loader)\n",
        "        finally:\n",
        "            wandb.finish()\n",
        "\n",
        "#call train function to train\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kCVTtFWAFgRf213te4rMyzfePkPAB04n",
      "authorship_tag": "ABX9TyNuXu1fmLXOUq9NIaN+EYIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}